import streamlit as st
import uuid
import json
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

# rag.py ã‹ã‚‰å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from rag import (
    graph,
    llm,
    initial_retrieval,
    cypher_planning,
    graph_execution,
    final_answer_prompt
)
from custom_history import OriginalNeo4jChatMessageHistory

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
@st.cache_data(show_spinner=False)
def generate_chat_title(_question: str) -> str:
    from langchain_core.prompts import PromptTemplate
    prompt = PromptTemplate.from_template("ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’ã€ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ã€ã”ãçŸ­ã„è¦ç´„ï¼ˆæœ€å¤§20æ–‡å­—ç¨‹åº¦ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚\n\nè³ªå•: {question}\n\nè¦ç´„ã‚¿ã‚¤ãƒˆãƒ«:")
    chain = prompt | llm | StrOutputParser()
    try:
        title = chain.invoke({"question": _question})
        return title.strip().replace('"', '').replace("'", "").replace("\n", " ")
    except Exception as e:
        print(f"ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return _question[:30] + "..."

def get_all_sessions():
    query = "MATCH (s:Session)-[:HAS_MESSAGE]->(latest_message:Message) ORDER BY latest_message.timestamp DESC RETURN s.id AS session_id, s.title AS title"
    try:
        results = graph.query(query)
        return [{"id": r['session_id'], "title": r.get('title') or f"ä¼šè©±-{r['session_id'][:8]}"} for r in results]
    except Exception as e:
        st.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def create_new_session_id():
    return str(uuid.uuid4())

def display_thinking_process(tp_data):
    """æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºã™ã‚‹é–¢æ•°"""
    st.markdown("##### 1. åˆæœŸæ¢ç´¢ (Hybrid Search)")
    st.code(tp_data.get("initial_context", "æƒ…å ±ãªã—"), language="text")
    st.markdown("##### 2. æ¢ç´¢è¨ˆç”» (Cypher Generation)")
    st.code(tp_data.get("cypher_query", "ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"), language="cypher")
    st.markdown("##### 3. ã‚°ãƒ©ãƒ•æ¢ç´¢çµæœ (JSON)")
    graph_context_str = tp_data.get("graph_context", "{}")
    try:
        # æ•´å½¢ã•ã‚ŒãŸJSONæ–‡å­—åˆ—ã‚’å†åº¦æ•´å½¢
        graph_context_formatted = json.dumps(json.loads(graph_context_str), indent=2, ensure_ascii=False)
    except (json.JSONDecodeError, TypeError):
        graph_context_formatted = graph_context_str
    st.code(graph_context_formatted, language="json")

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="Graph RAG Chat", layout="wide")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç† ---
if "session_id" not in st.session_state:
    all_sessions = get_all_sessions()
    st.session_state.session_id = all_sessions[0]['id'] if all_sessions else create_new_session_id()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼UI ---
with st.sidebar:
    st.title("ğŸ“„ ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³")
    if st.button("â• æ–°è¦ãƒãƒ£ãƒƒãƒˆ", use_container_width=True):
        st.session_state.session_id = create_new_session_id()
        st.rerun()
    st.divider()
    all_sessions = get_all_sessions()
    session_map = {s['id']: s['title'] for s in all_sessions}
    session_ids = [s['id'] for s in all_sessions]
    if st.session_state.session_id not in session_ids:
        session_ids.insert(0, st.session_state.session_id)
        session_map[st.session_state.session_id] = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
    try:
        current_session_index = session_ids.index(st.session_state.session_id)
    except ValueError:
        current_session_index = 0
    selected_session = st.selectbox("ãƒãƒ£ãƒƒãƒˆå±¥æ­´", options=session_ids, format_func=lambda session_id: session_map.get(session_id, session_id), index=current_session_index)
    if selected_session != st.session_state.session_id:
        st.session_state.session_id = selected_session
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆç”»é¢ ---
st.title("Graph RAG Chatbot")
current_title = session_map.get(st.session_state.session_id, "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ")
st.caption(f"ç¾åœ¨ã®ãƒãƒ£ãƒƒãƒˆ: {current_title}")

history = OriginalNeo4jChatMessageHistory(graph=graph, session_id=st.session_state.session_id)

# ===== ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ =====
for msg in history.messages:
    with st.chat_message(msg.type):
        # AIã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if msg.type == 'ai' and "thinking_process" in msg.additional_kwargs:
            with st.expander("æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹"):
                display_thinking_process(msg.additional_kwargs["thinking_process"])
        
        # æœ€çµ‚çš„ãªå›ç­”ã‚’è¡¨ç¤º
        st.markdown(msg.content)

# ===== ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å‡¦ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å…¨é¢çš„ã«ä¿®æ­£ =====
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.chat_message("user").markdown(prompt)

    state = {"question": prompt, "chat_history": history.messages}
    thinking_process_data = {}

    with st.chat_message("assistant"):
        # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¯st.statuså†…ã§å®Ÿè¡Œã—ã€å®Œäº†å¾Œè‡ªå‹•ã§é–‰ã˜ã‚‹
        with st.status("æ€è€ƒä¸­...") as status:
            try:
                # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«ä¿å­˜
                step1_result = initial_retrieval(state) #type: ignore
                status.write("âœ… åˆæœŸæ¢ç´¢å®Œäº†")
                step2_result = cypher_planning({**state, **step1_result}) #type: ignore
                status.write("âœ… æ¢ç´¢è¨ˆç”»å®Œäº†")
                step3_result = graph_execution({**state, **step1_result, **step2_result}) #type: ignore
                status.write("âœ… ã‚°ãƒ©ãƒ•æ¢ç´¢å®Œäº†")

                # å…¨ã¦ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã®è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
                thinking_process_data = {**step1_result, **step2_result, **step3_result}
                status.update(label="æ€è€ƒå®Œäº†ï¼", state="complete")

            except Exception as e:
                status.update(label="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", state="error")
                st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.stop()
        
        # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã—ãŸå¾Œã€æ°¸ç¶šçš„ãªUIã¨ã—ã¦è¡¨ç¤º
        with st.expander("æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹"):
            display_thinking_process(thinking_process_data)

        # æœ€çµ‚å›ç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
        final_answer_chain = final_answer_prompt | llm | StrOutputParser()
        history_str = "\n".join([f"  - {msg.type}: {msg.content}" for msg in state.get("chat_history", [])])
        chain_input = {**state, **thinking_process_data, "chat_history": history_str}
        full_response = st.write_stream(final_answer_chain.stream(chain_input))

    # --- ä¼šè©±å±¥æ­´ã®ä¿å­˜ã¨UIæ›´æ–° ---
    is_first_message = not history.messages
    history.add_user_message(prompt)
    
    # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚“ã AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ä¿å­˜
    ai_message_with_thinking = AIMessage(
        content=full_response,
        additional_kwargs={"thinking_process": thinking_process_data}
    )
    history.add_message(ai_message_with_thinking)

    if is_first_message:
        title = generate_chat_title(prompt)
        history.update_session_title(title)
        st.rerun()